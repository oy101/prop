Detection as Code (DaC) for Splunk ES in Production-Only Environments
SOC Analyst & Detection Engineer Training Plan

1. Operating Model: Detection as Code in Production
Environment Context:

All detection content is built, tested, and modified directly in the live production Splunk ES instance.

There is no development or staging environment.

Data required for detections exists only in production.

Detection logic is authored and version-controlled using YAML or JSON files but deployed and tested immediately in production.

Implications:

Detections must be safe by design.

All deployments must include manual testing steps.

Version control and human review are critical substitutes for automated QA environments.

2. Detection Lifecycle in a Live System
Author detection as a YAML or JSON file (includes SPL, metadata, risk scoring).

Submit for peer review using Git or an equivalent version control system.

Validate SPL manually in the search UI (preview only, non-impactful).

Apply the detection via REST API, command-line, or UI with alerts disabled initially.

Monitor output using dashboards, notables, and the | rest endpoint.

Enable alerting when behavior is confirmed.

3. Example Detection Artifact
yaml
Copy
Edit
name: excessive_failed_logins
enabled: false
search: |
  `cim_Authentication`
  | search action="failure"
  | stats count by user, src
  | where count > 10
schedule: cron: "*/15 * * * *"
alert_action:
  type: notable
  urgency: medium
  severity: high
risk:
  score: 70
  object_type: user
  object_field: user
annotations:
  mitre_attack: T1110
  tags: [brute_force, authentication]
4. Safe Practices for Detection Testing in Production
SPL Testing Methods:

Use | where false or | head 10 to safely preview search output.

Temporarily set enabled: false and alert.suppressed: true.

Limit search scope with index=, sourcetype=, or known test asset filters.

Route output to a summary or sandbox index using collect.

Manual SPL Example:

spl
Copy
Edit
`cim_Authentication`
| search action="failure"
| stats count by user, src
| where count > 10
| where false
5. Applying Detections in Production
REST API Example:

bash
Copy
Edit
curl -k -u admin:changeme \
  https://splunk:8089/servicesNS/nobody/SplunkEnterpriseSecuritySuite/saved/searches \
  -d name="excessive_failed_logins" \
  -d search="SPL content here" \
  -d disabled=true \
  -d alert.track=true
Alternative Methods:

Copy detection logic into savedsearches.conf via Splunk file system access.

Use Splunk Web UI under Content Management (only if governed).

6. SOC Analyst Responsibilities
Write or propose detection content as structured YAML.

Validate detection SPL manually before requesting activation.

Participate in peer reviews and provide feedback.

Monitor the behavior of newly deployed detections.

Tune rules to reduce false positives without suppressing true positives.

7. Training Modules
Module	Objective
1. Introduction to DaC	Understand the purpose and structure of Detection as Code in Splunk ES
2. Writing Detection Artifacts	Learn how to build and structure YAML/JSON for detections
3. Safe SPL Design	Develop SPL searches that minimize risk to production
4. Manual Deployment via API	Learn how to deploy searches using Splunk’s REST API
5. Tuning and Monitoring	Understand how to monitor rule output and reduce alert noise
6. Peer Review Process	Practice structured review of detection artifacts before activation

8. Hands-On Exercises
Lab	Activity
Author Detection	Write a YAML-based detection for a use case (e.g., excessive authentication failures)
Safe SPL Preview	Run the detection SPL in search with `
Deploy with Alert Suppression	Apply the detection via API with alerting disabled
Peer Review	Review a teammate’s detection rule and provide improvement suggestions
Monitor Rule Output	Use dashboards and `

9. Tools and Interfaces
GitHub or GitLab: Track detection artifact versions and peer reviews.

Splunk Search UI: Validate SPL searches interactively.

Job Inspector: Review search performance.

Splunk REST API: Deploy or update saved searches.

Notable Dashboards: Monitor detection behavior and alert frequency.

10. Best Practices Summary
Use strict filters and scoped fields in every SPL (index=, sourcetype=, tag=).

Always validate SPL manually before activation.

Use suppression or disable the alert initially until output is confirmed.

Store all detections in version-controlled repositories.

Document every detection with description, author, test steps, and MITRE mapping.

Regularly review detections for effectiveness, redundancy, and false positive trends.

11. Governance Enhancements (Optional)
Maintain a central registry of deployed detections with metadata (risk score, owner, last updated).

Establish a quarterly review of inactive or noisy rules.

Standardize a detection template in YAML for consistent formatting.

Automate alert frequency summaries to track effectiveness.
